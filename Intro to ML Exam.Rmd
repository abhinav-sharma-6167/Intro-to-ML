---
title: "Intro to ML Final Exam"
author: "Abhinav Sharma"
date: "7/24/2021"
always_allow_html: true
output:
  pdf_document: 
  html_document:
    df_print: paged
---

Setup - Loading libraries and setting working directory. Setting up custom ggplot theme for all plotting purposes.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#########################
##Cleaning Data and EDA##
#########################
#Install libraries if not installed, else load them-----------------------------
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
# usage
packages <- c("ggplot2", "ISLR", "DataExplorer", "RColorBrewer", "dplyr", "data.table","rpart","randomForest","xgboost","DescTools","Hmisc","ggcorrplot","MASS","tidyverse","caret","precrec","GGally","corrgram","broom","purrr","ISLR","glmnet","pls","leaps","ROCR","precrec","kknn","rpart","rpart.plot","gbm", "nnet","caret","neuralnet")
ipak(packages)

options(scipen=999)

#Set seed and working directory-------------------------------------------------
set.seed(100)
setwd("~/Documents/Intro to ML")
#, base_family="Avenir"
theme_custom <- function (text_size=12) { 
    theme_bw(base_size=text_size) %+replace% 
        theme(
            panel.background  = element_blank(),
            plot.background = element_rect(fill="gray96", colour=NA), 
            legend.background = element_rect(fill="transparent", colour=NA),
            legend.key = element_rect(fill="transparent", colour=NA)
        )
}

```


### Chapter #2 : Ques 10


#### Part (a)


```{r}
suppressMessages(suppressWarnings(attach(Boston)))
cat("Number of rows and columns are : ",nrow(Boston)," and ",ncol(Boston)," respectively.\n")
```
The data represents housing value in suburbs of Boston. Each row is one suburb and each column represents a particular characterstic associated with the particular suburb such as crime rate, taxation, accessibility to highways etc. The last column medv is median value of owner-occupied homes in units of $1000s.

#### Part (b)


```{r}
ggpairs(Boston[,c("crim","zn","indus","nox","ptratio","rad","tax","lstat","medv")],progress = F ,aes(alpha=0.6))+theme_custom(text_size =10)+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


We observe accessibility to radial highway (rad) and taxation (tax) have almost similar distribution with a very high positive correlation. Both these variables have some correlation with crime rate (crim) as well. 
The crime rate (crim) and proportion of residential zone (zn) variables are highly right skewed whereas proportion of non-retail business (indus) is bi-modal.

The scatterplots help us understand relationship of individual variables with median house value (medv) with it having inverse association with crime rate, NOx concentrations (nox), taxation (tax), percent of lower status population (lstat) whereas direct relatioship with proportion of residential zone (zn)

#### Part (c)


```{r}
#Generate correlation matrix
cor(Boston)

#Visualize correlation relationships
corrgram::corrgram(cor(Boston),upper.panel = panel.shade,lower.panel = NULL)

```


Above plot shows whether a pair of variable has direct or inverse association. The color of the tile indicated strength of association (correlation)

Observing the top row of the plot we see, crim has strong positive correlation with rad, tax and lstat, moderate positive correlation with indus, nox, age, ptratio. We observe moderate negative association with the variables zn, rm, dis, black and medv.

#### Part (d)



```{r}
cat("#Range of crime rate in Boston suburbs : ", range(crim)," \n\n")

cat("\nRange of tax rate in Boston suburbs : ", range(tax)," \n\n")

cat("\nRange of ptratio in Boston suburbs : ", range(ptratio)," \n\n")

```


```{r}
#Crim variable outliers
ggplot(Boston, aes(x=crim)) + geom_boxplot()+theme_custom()

#Tax rate variable outliers
ggplot(Boston, aes(x=tax)) + geom_boxplot()+theme_custom()

#Ptratio variable outliers
ggplot(Boston, aes(x=ptratio)) + geom_boxplot()+theme_custom()

```



We note ranges of the relevant predictors above. Crim shows outliers with higher than (mean+2sd) while ptratio shows outliers with lower values lower than (mean-2sd) No suburbs have particularly high values.
No outliers for tax variable.

While variable ptratio shows some left skew, crim shows very strong right skew indicating we might need some outlier treatment on crim as high values such as 88.97 may impact coefficients of regression and impact interpretation.

#### Part (e)


```{r}
table(chas)
#prop.table(table(chas))
```

A total of 35 suburbs (roughly 7%) are bounded by the Charles river.

#### Part (f)


```{r}
cat("Median pupil-teacher ratio in the dataset is :", median(ptratio),". \n")

```

#### Part (g)


```{r}

#Median values by variable
Med_df = data.frame(sapply(Boston,function(x) round(median(x),2)))
names(Med_df) <- "ColMedian"
Med_df$Range_l = sapply(Boston,function(x)  round(range(x)[1],2))
Med_df$Range_u = sapply(Boston,function(x)  round(range(x)[2],2))
Med_df

#Suburbs with minimum median value 
Boston[medv == min(medv),]

```


The suburbs in row number 399 and 406 have the lowest 'median value' of $5000. We see, compared to dataset medians these suburbs have very high crime rate, higher proportion of non-retail businesses, higher NOx concentration, lesser #rooms, higher distance from radial highways and are highly taxed.

Based on variable distributions and range, the crime rate, lstat and tax rates in these low median value suburbs are at the higher end of their respective distribution. 


#### Part (h)


```{r}
cat("#Suburbs averaging more than 7 dwellings : ",length(which(rm > 7)),". \n")
cat("#Suburbs averaging more than 8 dwellings : ",length(which(rm > 8)),". \n")

#Characteristics of suburbs with more than 8 dwellings on average
Sub_8 = data.frame(sapply(Boston[rm > 8,] ,function(x) round(mean(x),2)))
names(Sub_8) = "ColMean"
Sub_8
```
Suburbs with more than eight dwellings on average have very low crime rate (crim) and high proportion of residential land zones (zn) accompanied with better pupil-teacher ratios (ptratio) and lesser percent of lower status population (lstat)


### Chapter #3 : Ques 15

#### Part (a)


```{r}
#Running regression by variable
m1 = broom::tidy(summary(lm(crim~zn, data = Boston)),conf.int=T)
m2 = broom::tidy(summary(lm(crim~indus, data = Boston)),conf.int=T)
m3 = broom::tidy(summary(lm(crim~chas, data = Boston)),conf.int=T)
m4 = broom::tidy(summary(lm(crim~nox, data = Boston)),conf.int=T)
m5 = broom::tidy(summary(lm(crim~rm, data = Boston)),conf.int=T)
m6 = broom::tidy(summary(lm(crim~age, data = Boston)),conf.int=T)
m7 = broom::tidy(summary(lm(crim~dis, data = Boston)),conf.int=T)
m8 = broom::tidy(summary(lm(crim~rad, data = Boston)),conf.int=T)
m9 = broom::tidy(summary(lm(crim~tax, data = Boston)),conf.int=T)
m10 = broom::tidy(summary(lm(crim~ptratio, data = Boston)),conf.int=T)
m11 = broom::tidy(summary(lm(crim~black, data = Boston)),conf.int=T)
m12 = broom::tidy(summary(lm(crim~lstat, data = Boston)),conf.int=T)
m13 = broom::tidy(summary(lm(crim~medv, data = Boston)),conf.int=T)

coef_df <- as.data.frame(rbind(m1[2,],m2[2,],m3[2,],m4[2,],m5[2,],m6[2,],m7[2,],m8[2,],m9[2,],m10[2,],m11[2,],m12[2,],m13[2,]))
coef_df[!names(coef_df) %in% c("term")] <- sapply(coef_df[!names(coef_df) %in% c("term")] , function(x) round(x,2))
coef_df
```


All variables except for chas (bound by Charles river) return significant coefficients for predicting crime rate. Variables zn, rm, dis returns negative slope, whereas indus, nox, age, rad, tax and ptratio return significant positive betas. Variables rad and tax return highest absolute values of t-statistic implying lower p-values and greater confidence on the betas.

```{r results='hide'}
#ggplot(data = Boston,aes(x = lstat, y = crim)) + scale_x_continuous(name="lstat", limits=range(lstat)) +
#scale_y_continuous(name="crim", limits=range(crim)) +
#scale_linetype(name="s") + geom_point(color="salmon",size=2.1,alpha=0.35)+theme_custom() + 
#geom_smooth(method = "lm",color="blue",linetype=4, size = 0.65)

plotFunction <- function(varname1){            
  plot(suppressMessages(suppressWarnings(ggplot(Boston, aes_string(x=varname1, y="crim")) + geom_point(color="salmon",size=2.1,alpha=0.35)+theme_custom() + 
geom_smooth(method = "lm",color="blue",linetype=4, size = 0.65))))
}    

list_plots <- sapply(names(Boston)[!names(Boston) %in% c("crim")], plotFunction)
```

Above plot shows regression lines with respect to each variable and crim - blue one with OLS estimates and band of grey indicating confidence interval. 

As we can see from the plot,we have atleast 95% confidence that the slope of the regression line is non-zero for all regression line except chas which has upper and lower confidence bands with positive and negative slopes respectively implying we can't satisfactorily say that slope isn't zero. 

This proves aforementioned statistically ignificant association of all remaining variables.


#### Part (b)
```{r}
lm.model <- lm(crim ~ ., data = Boston)
summary(lm.model)

```

We can reject the null hypothesis (beta = 0) for the variables zn, dis, rad, black and medv. We observe some betas may have changed directionality from their initial results in simple regression.

#### Part (c)
```{r}
both_coef <- data.frame(Simple = round(coef_df$estimate,2), Multiple = round(lm.model$coefficients[2:length(lm.model$coefficients)],2)  )
both_coef
```

We observe directionality of coefficients changes for zn, indus, nox, rm, ptratio when all variables are considered together.

```{r}
ggplot(data = both_coef,aes(x = Simple, y = Multiple)) + geom_point(color="red",fill="black",size=2.5,alpha=0.5)+theme_custom()
```
Coefficient for nox sees most deviation, changing from 31 in simple lr to -10 in multiple regression. These changes are due to the fact that multiple regression provides coefficients under the condition that all remaining covariates are held constant whereas simple linear regression only takes in consideration the correlation between the dependent and independent variable.


#### Part (d)
```{r warning=FALSE}
#To check for non-linearity -- Approach 1 -- Plot
suppressMessages(ggplot(data = Boston,aes(x = zn, y = crim)) + geom_point(color="salmon",fill="black",size=1.5,alpha=0.65)+theme_custom()+ylim(c(0,10))+
  geom_smooth(method = "lm",se=F,color="black",size=0.8)+geom_smooth(method = "loess",se=F,color="blue",linetype=2,size=0.65)+geom_smooth(method = "gam",se=F,color="darkgreen",linetype=4,size=0.65)) #Only crim values less than 10 for better visual


```
Plotting variables and fitting local polynomial regression fitting or generalized additive models we see there is some non-linearity present in relationship of crim with zn.

```{r}
#Approach 2 -- Pattern in residuals
mod_summ <- summary(lm(crim~zn))
plot(zn, mod_summ$residuals  , ylab="Residuals", col="salmon")

```
We see non-linearity in residual vs independent variable plot meaning linear model has not been able to extract the non-linear aspect of the relationship between crim and zn.

```{r}
#Approach 3 -- Running polynomial regression
#Running regression by variable
p1 = broom::tidy(summary(lm(crim~poly(zn,3))),conf.int=T)
p2 = broom::tidy(summary(lm(crim~poly(indus,3))),conf.int=T)
p3 = broom::tidy(summary(lm(crim~poly(chas,1))),conf.int=T)
p4 = broom::tidy(summary(lm(crim~poly(nox,3))),conf.int=T)
p5 = broom::tidy(summary(lm(crim~poly(rm,3))),conf.int=T)
p6 = broom::tidy(summary(lm(crim~poly(age,3))),conf.int=T)
p7 = broom::tidy(summary(lm(crim~poly(dis,3))),conf.int=T)
p8 = broom::tidy(summary(lm(crim~poly(rad,3))),conf.int=T)
p9 = broom::tidy(summary(lm(crim~poly(tax,3))),conf.int=T)
p10 = broom::tidy(summary(lm(crim~poly(ptratio,3))),conf.int=T)
p11 = broom::tidy(summary(lm(crim~poly(black,3))),conf.int=T)
p12 = broom::tidy(summary(lm(crim~poly(lstat,3))),conf.int=T)
p13 = broom::tidy(summary(lm(crim~poly(medv,3))),conf.int=T)

coef_poly_df <- as.data.frame(rbind(p1[2:4,],p2[2:4,],p3[2,],p4[2:4,],p5[2:4,],p6[2:4,],p7[2:4,],p8[2:4,],p9[2:4,],p10[2:4,],p11[2:4,],p12[2:4,],p13[2:4,]))

coef_poly_df[!names(coef_poly_df) %in% c("term")] <- sapply(coef_poly_df[!names(coef_poly_df) %in% c("term")] , function(x) round(x,4))

coef_poly_df
```
### Chapter #6 : Ques 9

#### Part (a)

```{r}
#Load data
college <- ISLR::College

#sapply(train,class)
#Since we have only one factor, we coerce it to numeric
college$Private <- as.numeric(as.factor(college$Private)) - 1

#One represents Private = 'Yes' now


#Reproducibility
set.seed(100)

# 75% of the sample size
smp_size <- floor(0.75 * nrow(college))
train_ind <- sample(seq_len(nrow(college)), size = smp_size)

train <- college[train_ind, ]
test <- college[-train_ind, ]

#Dimensions of train and test
dim(train) ; dim(test)

#Number of colleges overlap in train and test
length(which(rownames(train) %in% rownames(test)))

#Scale original data
#college_scaled = as.data.frame(sapply(college,scale))

#Create copies of train test with scaled data
#train_scaled <- college_scaled[train_ind, ]
#test_scaled <- college_scaled[-train_ind, ]

```

Train test split made with train set having 75% of datapoints and remaining in test set. Dependent variable is Apps. We do not see any overlap in train and test rownames.

#### Part (b)

```{r}

set.seed(100)

linearModel <- lm(Apps~. , data = college)
summary(linearModel)


predApps <- predict(linearModel , test)

#Evaluate deviation from actual y
errorApps = test$Apps - predApps
RMSE_error = sqrt(mean((test$Apps - predApps)^2))

cat("Test Root mean squared error is : ",RMSE_error , "\n")

cat("Train RMSE is :",sqrt(mean((train$Apps - predict(linearModel , train))^2))," \n")

```

We see train RMSE is bit higher than test set RMSE.

#### Part (c)

```{r}

#Creating independent and dependent variables in required classes
x.train <- as.matrix(train[names(train)[!names(train) %in% c("Apps")]])
x.test <- as.matrix(test[names(test)[!names(test) %in% c("Apps")]])

y.train <- train$Apps
y.test <- test$Apps

#Using unscaled data since glmnet implementation standardizes data internally as stated on https://www.statology.org/ridge-regression-in-r/

set.seed(100)

cv_Ridge <- cv.glmnet(x = x.train, y = y.train , family = "gaussian" , alpha=0)

plot(cv_Ridge)

```
```{r}
set.seed(100)

cv_Ridge_min <- glmnet(x = x.train, y = y.train , family = "gaussian", lambda = cv_Ridge$lambda.min)


predApps_Ridge <- predict(cv_Ridge_min , x.test)

#Evaluate deviation from actual y
errorApps_Ridge = test$Apps - predApps_Ridge
RMSE_error_Ridge = sqrt(mean((errorApps_Ridge)^2))

cat("Test Root mean squared error for Ridge regularized model is : ",RMSE_error_Ridge , "\n")

```

Ridge regression reduces test set RMSE marginally.


#### Part (d)

```{r}
set.seed(100)

cv_Lasso <- cv.glmnet(x = x.train, y = y.train , family = "gaussian" , alpha=1)

plot(cv_Lasso)

cv_Lasso_min <- glmnet(x = x.train, y = y.train , family = "gaussian", lambda = cv_Lasso$lambda.min)


predApps_Lasso <- predict(cv_Lasso_min , x.test)

#Evaluate deviation from actual y
errorApps_Lasso = test$Apps - predApps_Lasso
RMSE_error_Lasso = sqrt(mean((errorApps_Lasso)^2))

cat("Test Root mean squared error for Lasso regularized model is : ",RMSE_error_Lasso , "\n")

coef_df_Lasso = data.frame(VarName = rownames(as.matrix(coef(cv_Lasso_min)))  , as.matrix(coef(cv_Lasso_min)) )

names(coef_df_Lasso)[2] <- "Beta"
coef_df_Lasso$Beta <- round(coef_df_Lasso$Beta,2)

coef_df_Lasso[coef_df_Lasso$Beta != 0,]
```
Lasso regression further reduces test set RMSE a little bit.

#### Part (e)

```{r}
# PCR model by cross validation
set.seed(100)

pcr.fit = pls::pcr(formula = Apps~. , data=train, scale=TRUE ,validation ="CV")

summary(pcr.fit)

validationplot(pcr.fit ,val.type="MSEP")

```
Given the mean squared error of prediction saturates at number of components = 9, we choose M = 9 for our final model.

```{r}
set.seed(100)

pcr.fit2 = pls::pcr(formula = Apps~. , data=train, scale=TRUE ,ncomp= 9 )

pls.pred = predict (pcr.fit2 , test)

#Evaluate deviation from actual y
errorApps_PCR = test$Apps - pls.pred
RMSE_error_PCR = sqrt(mean((errorApps_PCR)^2))

cat("Test Root mean squared error for PCR  model is : ", RMSE_error_PCR , "\n")

```

Even upon choosing M via cross-validation, PCR model gives worse RMSE than Ridge or Lasso.

#### Part (f)
```{r}
set.seed(100)

pls.fit2 = plsr(formula = Apps~. , data=train, scale=TRUE ,validation ="CV")

validationplot(pls.fit2 ,val.type="MSEP")

pls.pred2 = predict(pls.fit2 ,test ,ncomp =5)

#Evaluate deviation from actual y
errorApps_PLS = test$Apps - pls.pred2
RMSE_error_PLS = sqrt(mean((errorApps_PLS)^2))

cat("Test Root mean squared error for PLS model is : ", RMSE_error_PLS , "\n")
```

PLS model performs better than pricipal component regression but is still not able to beat RMSE scores from Ridge and Lasso on test set.


#### Part (g)

```{r}

data.table(Model = c("LR","Ridge","Lasso","PCR","PLS") , RMSE = c(RMSE_error,RMSE_error_Ridge,RMSE_error_Lasso,RMSE_error_PCR, RMSE_error_PLS))

```

We see Lasso despite omitting out few variables gives us a better RMSE than all other models. Ridge regularization also gives improvement over the linear regression RMSE. Interestingly, there's quite a difference between RMSE values of PCR and PLS despite both of them being based on dimensionality reduction while greedily maximing variance. This implies that the supervised transformations that PLS does post PCA offer good predictive power in estimating dependent variable.

### Chapter #6 : Ques 11

#### Part (a)

```{r}
#Reproducibility
set.seed(100)

# 75% of the sample size
smp_size <- floor(0.75 * nrow(Boston))
train_ind <- sample(seq_len(nrow(Boston)), size = smp_size)

train <- Boston[train_ind, ]
test <- Boston[-train_ind, ]

#Creating independent and dependent variables in required classes
x.train <- as.matrix(train[names(train)[!names(train) %in% c("crim")]])
x.test <- as.matrix(test[names(test)[!names(test) %in% c("crim")]])

y.train <- train$crim
y.test <- test$crim

#Creating independent and dependent variables in required classes
x.train <- as.matrix(train[names(train)[!names(train) %in% c("crim")]])
x.test <- as.matrix(test[names(test)[!names(test) %in% c("crim")]])

y.train <- train$crim
y.test <- test$crim

#Using unscaled data since glmnet implementation standardizes data internally as stated on https://www.statology.org/ridge-regression-in-r/

set.seed(100)

cv_Ridge <- cv.glmnet(x = x.train, y = y.train , family = "gaussian" , alpha=0)

plot(cv_Ridge)

set.seed(100)

cv_Ridge_min <- glmnet(x = x.train, y = y.train , family = "gaussian", lambda = cv_Ridge$lambda.min)


predcrim_Ridge <- predict(cv_Ridge_min , x.test)

#Evaluate deviation from actual y
errorcrim_Ridge = test$crim - predcrim_Ridge
RMSE_error_Ridge = sqrt(mean((errorcrim_Ridge)^2))

cat("Test Root mean squared error for Ridge regularized model is : ",RMSE_error_Ridge , "\n")

set.seed(100)

cv_Lasso <- cv.glmnet(x = x.train, y = y.train , family = "gaussian" , alpha=1)

plot(cv_Lasso)

cv_Lasso_min <- glmnet(x = x.train, y = y.train , family = "gaussian", lambda = cv_Lasso$lambda.min)


predcrim_Lasso <- predict(cv_Lasso_min , x.test)

#Evaluate deviation from actual y
errorcrim_Lasso = test$crim - predcrim_Lasso
RMSE_error_Lasso = sqrt(mean((errorcrim_Lasso)^2))

cat("Test Root mean squared error for Lasso regularized model is : ",RMSE_error_Lasso , "\n")

coef_df_Lasso = data.frame(VarName = rownames(as.matrix(coef(cv_Lasso_min)))  , as.matrix(coef(cv_Lasso_min)) )

names(coef_df_Lasso)[2] <- "Beta"
coef_df_Lasso$Beta <- round(coef_df_Lasso$Beta,2)

coef_df_Lasso[coef_df_Lasso$Beta != 0,]

# PCR model by cross validation
set.seed(100)

pcr.fit = pls::pcr(formula = crim~. , data=train, scale=TRUE ,validation ="CV")

summary(pcr.fit)

validationplot(pcr.fit ,val.type="MSEP")

set.seed(100)

pcr.fit2 = pls::pcr(formula = crim~. , data=train, scale=TRUE ,ncomp= 9 )

pls.pred = predict (pcr.fit2 , test)

#Evaluate deviation from actual y
errorcrim_PCR = test$crim - pls.pred
RMSE_error_PCR = sqrt(mean((errorcrim_PCR)^2))

cat("Test Root mean squared error for PCR  model is : ", RMSE_error_PCR , "\n")

#using max number of variables based on regfit
regFitModel <- regsubsets(crim~. , data = Boston , nvmax=15, method ="forward")

plot(regFitModel)$adjr2

#Adjusted R-sq for model subset -- Model removing chas, rm, age and tax
summary(regFitModel)$adjr2[ (which(summary(regFitModel)$adjr2 == max(summary(regFitModel)$adjr2)))]

set.seed(100)

reglinearModel <- lm(crim~zn+indus+nox+dis+rad+ptratio+black+lstat+medv , data = Boston)
summary(reglinearModel)

predcrim <- predict(reglinearModel , test)

#Evaluate deviation from actual y
errorcrim = test$crim - predcrim
RMSE_error = sqrt(mean((test$crim - predcrim)^2))

cat("Test Root mean squared error is : ",RMSE_error , "\n")

data.table(Model = c("Subsetting","Ridge","Lasso","PCR") , RMSE = c(RMSE_error,RMSE_error_Ridge,RMSE_error_Lasso,RMSE_error_PCR))
```
We see best subsetting offers us lowest test set RMSE. From the percent deviation optimization plots of ridge and Lasso, we see that the variance does not drastically change for different values of lambda meaning both L1-L2 regularizations are not able to easily generalize the regression model. Now given Principal component regression is also not able to get better test RMSE, it suggests the actual subset of columns is better able to model unseen data than orthogonal principal components attempting to explain most variance in data.

This could probably be due to relatively high correlation of target variables with x. The orthogonal PCs might be missing out on some correlation and essentially have slightly more information loss.

Hence, despite lasso and PCR being good tools, in theory to weed out unnecessary information and generate stable robust models, in this case, I'd prefer the 'best subset' model as validated by test RMSEs above.


#### Part (b)

It might be interesting to see how PCLS - using supervised transformation of pricipal components explains variance in the data and if it is able to beat the Best subset RMSE benchmark.
```{r}

set.seed(100)

pls.fit2 = plsr(formula = crim~. , data=Boston, scale=TRUE ,validation ="CV")

validationplot(pls.fit2 ,val.type="MSEP")

pls.pred2 = predict(pls.fit2 ,test ,ncomp =5)

#Evaluate deviation from actual y
errorApps_PLS = test$crim - pls.pred2
RMSE_error_PLS = sqrt(mean((errorApps_PLS)^2))

cat("Test Root mean squared error for PLS model is : ", RMSE_error_PLS , "\n")
```

Very interesting to observe, the supervised transformations PLS model is conducting upon the principal components is able to perform better than the previous best, 'best subset' model. The improvement is although highly sensitive to the number of principal components we avail. If we use lesser number of principal components we essentially risk losing out on the information contained in the variables. On the contrary, if we assume more principal components we have the risk of our model being not generalized enough for test data.

#### Part (c)

The two models that perform best on the hold-out sample are the best subset model and the Partial least squares based on Principal Components. Both variables do not leverage all the variables and the reduction in number of variables helps reduce excess variance in the model, thereby providing robust predictions.

The Best subset method drops out chas, rm, age and tax. Chas is not significant in linear model as well implying the variable might not have the signals to aid in increasing predictive power. Rm almost follow a gaussian distribution. All variables rm, age and tax may have been removed since their effect might have been captured through other correlated variables and adding them ends up over-complicating the model. Similarly, PLS model gives similar accuracy out-of-sample with 6 principal components, maybe reducing confounding effect of independent variables with each other. Compared to the 9-variable best-subset it is a simpler model but loses on model interpretability.

To summarize, if model interpretation is a necessity, the preffered model would be Best subset model vs if accuracy on out-of-sample data is key priority for our model, the PLS model holds best promise of a simpler, robust model.


### Chapter #4 : Ques 10

#### Part (a)


```{r}
df <- ISLR::Weekly

#Descriptive stats and univariate plots
DescTools::Desc(df)

```
```{r}

#Coerce Direction into numeric
df$Direction <- as.numeric(df$Direction) - 1
#Now 0 indicates down and 1 indicates Up

#Visualize correlation relationships
corrgram::corrgram(cor(df),upper.panel = panel.shade,lower.panel = NULL)

```
```{r}

#Pair plots to understand scatterplot distributions
ggpairs(df,progress = F ,aes(alpha=0.6))+theme_custom(text_size =10)+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


While univariate plots for all lag variables seem very much the same in terms of the shape of their distributions, we see there is no overall correlation amidst them. Volume variable is highly correlated with Year which makes sense since there wouldn't be too much variation in terms of total volume traded in a given year. Good years should have higher volumes traded, whereas recession periods should have lesser volumes traded.

Direction is highly correlated with the variable Today given Direction (current week up or down) is derived from the variable Today (current week percentage return)

Volume seems to be consistently increasing with the years!

#### Part (b)

```{r}
set.seed(100)

logitModel = glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=df, family=binomial)
summary(logitModel)

```

In addition to the Intercept only the Lag2 variable (% Return of last 2 weeks) gives out significant co-efficient.

#### Part (c)

```{r}

PredAll = predict(logitModel, newdata=df[,!colnames(df) == "Direction"], type="response")
confusionMatrix(as.factor(as.numeric(PredAll>=0.5)),reference = as.factor(df$Direction))

#Zero represents Down, 1 represents Up
logit_pred_obj <- prediction(PredAll,df$Direction)
auc_logit_all <- performance(logit_pred_obj,"auc")

#AUC of the model
auc_logit_all@y.values[[1]][1]

#AUC ROC and PR Curve
precrec_obj_test <- evalmod(scores = PredAll, labels = df$Direction)
autoplot(precrec_obj_test)

logit_pr_all <- performance(logit_pred_obj,"rec","prec")
pr_df= data.frame(logit_pr_all@x.values,logit_pr_all@y.values)
names(pr_df) <- c("Precision", "Recall")
pr_df$F1 = 2*pr_df$Precision * pr_df$Recall / (pr_df$Precision + pr_df$Recall)



```

We see not a great model with an AUC 0.55, implying the model is not able to segregate the two classes apart from each other as in evitable from the ROC curve. The model with 0.5 cutoff predicts majority datapoints as ones with Upper Direction giving out high amount of False Positives, thereby a very low precision despite good enough recall.

Since $Accuracy = \frac{(TP+TN)}{(TP+TN+FP+FN)}$ we have overall Accuracy as 0.5611. We also know that Precision is fraction of total predictions that landed correctly. With $Precision = \frac{TP}{(TP+FP)}$ we avail precision as 557/(557+430) = 0.5643. As stated earlier, this low precision is since our models predicts majority of observations as Up, thereby false positives or Type I error is high.

We also see recall availed as $Recall = \frac{TP}{TP+FN}$ thereby dependent upon False Negatives or Type II error. Recall = Specificity = 0.92 which is good but at a very high price of bad precision.


#### Part (d)
```{r}
#Create out train and test
train_df = df[(df$Year<2009),]

test_df = df[(!df$Year<2009),]

set.seed(100)

# Logistic regression on train
logitModel_Lag2 = glm(Direction ~ Lag2, data=train_df, family=binomial)

# Predictions on test
probLags2 = predict(logitModel_Lag2,test_df[,!colnames(test_df) == "Direction"], type="response")

confusionMatrix(as.factor(as.numeric(probLags2>=0.5)),reference =as.factor(test_df$Direction))

#Zero represents Down, 1 represents Up
logit_pred_obj2 <- prediction(probLags2,test_df$Direction)
auc_logit_all2 <- performance(logit_pred_obj2,"auc")

#AUC of the model
auc_logit_all2@y.values[[1]][1]

#AUC ROC and PR Curve
precrec_obj_test2 <- evalmod(scores = probLags2, labels = test_df$Direction)
autoplot(precrec_obj_test2)

logit_pr_all <- performance(logit_pred_obj2,"rec","prec")
pr_df= data.frame(logit_pr_all@x.values,logit_pr_all@y.values)
names(pr_df) <- c("Precision", "Recall")
pr_df$F1 = 2*pr_df$Precision * pr_df$Recall / (pr_df$Precision + pr_df$Recall)


```
The problem of overpredicting the class Up persists with overall accuracy = 0.625, recall = 0.918 but a low precision of 0.622

The ROC curve is intersecting the x = y line implying that the model often performs worse than a random classifier implying there is too much uncertainty associated with the predictions of the model at different probability cutoffs with cetain time when model performs worse than a random classifier. This is interesting given the 'statistically significant' variable Lags2 is not sufficient in itself or does not have adequate patterns to be able to segregate the target classes on it's own when tested on the out-of-sample data.

#### Part (g)

```{r}

# Predictions for K=1
set.seed(100)

knnModel = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=1)

#Confusion matrix for the test set
confusionMatrix(as.factor(knnModel$fitted.values),reference = as.factor(test_df$Direction))

#AUC ROC and PR Curve
#precrec_obj_test3 <- evalmod(scores = knnModel$fitted.values, labels = test_df$Direction)
#autoplot(precrec_obj_test3)

```

We observe a decrease in the accuracy and the model is not being able to pick some signals up relevant to segregating Upwards vs Downwards. Accuracy = 0.4615, Recall = 0.3770 and Precision drops as well due to reduction in False positives at 0.5609

#### Part (h)

KNN with k = 1 (1st nearest neighbour) performs better than the logistic regression on the out-of-sample data.

This is expected as the most dominant predictor of a time series data, especially in trading data would be the nearest trends surrounding the market. For example, if the market is going down for few consistent weeks, the knn model will be able to pick up on this method while searching for the 'neighborhood'

The only catch with knn, would be inability to predict sharp turns from 'Down to Up' or 'Up to Down' since historical data would always recommend the most occuring trend for particular values of Lags, especially for low values of k and ideally we shouldn't be using future data to predict the past as it'll be an obvious leakage.


#### Part (i)


```{r warning=FALSE}
#Exploring stepwise models to see effect of addition of variables
set.seed(100)


step.model <- glm(Direction ~ ., data=train_df[!names(train_df) %in% c("Today")], family=binomial) %>% stepAIC(trace = T)
cols <- names(coef(step.model))
summary(step.model)

PredT = predict(step.model, newdata=test_df[!names(test_df) %in% c("Today")], type="response")
confusionMatrix(reference = as.factor(test_df$Direction), as.factor(as.numeric(PredT > 0.5)))


```

We avail an accuracy of 0.5769 with a precision of 0.5955 and recall of 0.8688.

Only Lag1 and Lag2 variables are kept in stepwise regression. But Lag1 and Lag2 despite being uncorrelated might have a dependency upon each other. It could be worthwhile to explore the interaction effects associated with them.

```{r}

#Exploring interaction effects
set.seed(100)

interactionModel <- glm(Direction ~ Lag1 + Lag2 + Lag1:Lag2, data=train_df[!names(train_df) %in% c("Today")], family=binomial) 
cols <- names(coef(interactionModel))
summary(interactionModel)

PredT2 = predict(interactionModel, newdata=test_df[!names(test_df) %in% c("Today")], type="response")
confusionMatrix(reference = as.factor(test_df$Direction), as.factor(as.numeric(PredT2 > 0.5)))


```

We see no change in either the predictive power of any significant p-value for any interaction effects in Lag variables. Essentially this means, that of all the information given, Lag1 and Lag2 have the most useful information while their effects are independent of the value of the other variable.

To further improve predictive power, we might need to try other transformations or algorithms.

```{r}

#Model non-linearity of variables
set.seed(100)

nonLinModel <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data=train_df[!names(train_df) %in% c("Today")], family=binomial) 
cols <- names(coef(nonLinModel))
summary(nonLinModel)

PredT3 = predict(nonLinModel, newdata=test_df[!names(test_df) %in% c("Today")], type="response")
confusionMatrix(reference = as.factor(test_df$Direction), as.factor(as.numeric(PredT3 > 0.5)))


```
Again, we do not see any improvement by including quadratic transformations of pertinent Lag values. Our hope id with KNN and better Ks, the model is also able to map the changes from Up to Down and vice versa better than the k=1 model. Testing out different k-values :

```{r}
#KNN with k = 5, 10, 20, 50, 100

set.seed(100)

knnModel5 = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=5)
#Confusion matrix for the test set
confusionMatrix(as.factor(as.numeric(knnModel5$fitted.values > 0.5)),reference = as.factor(test_df$Direction))

set.seed(100)

knnModel10 = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=10)
#Confusion matrix for the test set
confusionMatrix(as.factor(as.numeric(knnModel10$fitted.values > 0.5)),reference = as.factor(test_df$Direction))

set.seed(100)

knnModel20 = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=20)
#Confusion matrix for the test set
confusionMatrix(as.factor(as.numeric(knnModel20$fitted.values > 0.5)),reference = as.factor(test_df$Direction))

set.seed(100)

knnModel50 = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=50)
#Confusion matrix for the test set
confusionMatrix(as.factor(as.numeric(knnModel50$fitted.values > 0.5)),reference = as.factor(test_df$Direction))


set.seed(100)

knnModel100 = kknn(Direction~Year+Lag1+Lag2+Lag3+Lag4+Lag5+Volume, train_df, test_df, k=100)
#Confusion matrix for the test set
confusionMatrix(as.factor(as.numeric(knnModel100$fitted.values > 0.5)),reference = as.factor(test_df$Direction))



```

With k = 100 we are able to achieve relatively higher accuracy = 0.5673 with low precision of 0.5975 and recall of 0.8033

We're able to push for marginally higher Precision compared to logistic regression but overall accuracy and recall are still lacking!

### Chapter #8 : Ques 8

#### Part (a)

```{r}
cs <- ISLR::Carseats

#Reproducibility
set.seed(100)

# 75% of the sample size
smp_size <- floor(0.75 * nrow(cs))
train_ind <- sample(seq_len(nrow(cs)), size = smp_size)

train <- cs[train_ind, ]
test <- cs[-train_ind, ]

cat("Number of rows of train and test are :" , nrow(train) , " and " , nrow(test)," respectively.\n" )

```



#### Part (b)

```{r}
tree1 = rpart(Sales ~ . , data = train , method = "anova")
tree1

rpart.plot::rpart.plot(tree1, box.palette = "Greens")
```

From the tree we can interpret, characteristics conducive to sales are having Good Shelveloc, being more priced
higher as well and greater Age and marketing spends. A combination of where these characterstics intersect offers much better value for Sales.


```{r}

predTree <- predict(tree1 , test)
cat("Mean Squared Error on the test set is :", mean((predTree - test$Sales)^2))

```


#### Part (c)

```{r}

printcp(tree1)
plotcp(tree1)

```
The printcp and plotcp methods in rpart cross-validate and return the cross-validated relative error with respect to the number of splits and complexity parameter, along with the recommendation (dashed horizontal line) of which minimum error or which split and complexity parameter would work out best for a given tree. 

Here ideal cp is 0.02-0.038 with 5-10 splits!


```{r}
#using cp = 0.038
tree2 = rpart(Sales ~ . , data = train , method = "anova" , cp = 0.02)

rpart.plot::rpart.plot(tree2, box.palette = "Greens")


predTree2 <- predict(tree2 , test)
cat("Mean Squared Error of pruned tree on the test set is :", mean((predTree2 - test$Sales)^2))

```
Using a pruned tree with 10 leaf nodes at a complexity parameter of 0.02, we're able to slightly reduce the test set MSE from 4.565 to 4.335. However if we keep on pruning, after certain time, we might underfit the model and MSE might rise again.


#### Part (d)

```{r}

# Bagging => RF with m = ncol
set.seed(100)

bagged10 = randomForest(Sales~., data=train, mtry=10, importance=T)

#importance Matrix
importance(bagged10)
varImpPlot(bagged10)

baggedPred10 = predict(bagged10,newdata = test)
cat("Mean Squared Error of RF with m = 10 on the test set is :",mean((baggedPred10-test$Sales)^2),"\n")


```
Similar to the rpart plots, top features are ShelveLoc, Price and CompPrice followed by Age and advertising that were key to certain child nodes in the rpart tree!

We see RF is able to considerably reduce the out-of-sample MSE (2.17) on account of bagging multiple samples with replacement. 

```{r}

# RandomForest (m = 2)
set.seed(100)

bagged2 = randomForest(Sales~., data=train, mtry=2, importance=T)

#importance Matrix
importance(bagged2)
varImpPlot(bagged2)

baggedPred2 = predict(bagged2,newdata = test)
cat("Mean Squared Error of RF with m = 2 on the test set is :",mean((baggedPred2-test$Sales)^2),"\n")




# RandomForest (m = 4)
set.seed(100)

bagged4 = randomForest(Sales~., data=train, mtry=4, importance=T)

#importance Matrix
importance(bagged4)
varImpPlot(bagged4)

baggedPred4 = predict(bagged4,newdata = test)
cat("Mean Squared Error of RF with m = 4 on the test set is :",mean((baggedPred4-test$Sales)^2),"\n")




# RandomForest (m = 6)
set.seed(100)

bagged6 = randomForest(Sales~., data=train, mtry=6, importance=T)

#importance Matrix
importance(bagged6)
varImpPlot(bagged6)

baggedPred6 = predict(bagged6,newdata = test)
cat("Mean Squared Error of RF with m = 6 on the test set is :",mean((baggedPred6-test$Sales)^2),"\n")



# RandomForest (m = 8)
set.seed(100)

bagged = randomForest(Sales~., data=train, mtry=8, importance=T)

#importance Matrix
importance(bagged)
varImpPlot(bagged)

baggedPred = predict(bagged,newdata = test)
cat("Mean Squared Error of RF with m = 8 on the test set is :",mean((baggedPred-test$Sales)^2),"\n")

```

The top 2 variables are constant for all values of m, viz. Shelveloc and Price in order. The next three important variables are Price, Age and Advertising as seen above, but the order of importance changes with different values of m.

MSEs for different m are comparable but none of the m values are able to beat the bagging benchmark test MSE.

As m keeps on increasing, the test MSE in this case keeps on decreasing.

### Chapter #8 : Ques 11

#### Part (a)

```{r}
carv <- ISLR::Caravan

carv$Purchase <- as.numeric(carv$Purchase)-1
#Now No is represented by 0 and Yes by 1

carv.train <- carv[1:1000,]
carv.test <- carv[1001:nrow(carv),]

cat("Number of rows in train and test are : ",nrow(carv.train)," and ",nrow(carv.test)," respectively.\n")
```


#### Part (b)

```{r warning=FALSE}

set.seed(100)

#Boosting
boostingModel = gbm(Purchase~., data=carv.train,distribution = "bernoulli",n.trees = 1000, shrinkage = 0.01)
summary(boostingModel,cBars = 25,
  method = relative.influence, # also can use permutation.test.gbm
  las = 1)
```

Few top variables explaining dependent variable best are :
PPERSAUT > MKOOPKLA > MOPLHOOG > PBRAND > MBERMIDD at the given hyperparameters


#### Part (c)


```{r}
# Predcited probalbilites on test
predCarv = predict(boostingModel,  carv.test[!names(carv.test) %in% c("Purchase")], n.trees = 1000,type="response")

# Confusion matrix
confusionMatrix(reference = as.factor(carv.test$Purchase), as.factor(as.numeric(predCarv >= 0.2)))

```

Proportion of people predicted to purchase who actually made one, equal model precision where $Precision = \frac{TP}{(TP+FP)}$ = $\frac{34}{(34+120)}$ = 0.2207

Even though overall accuracy is high, we see very low precision and recall metrics for this model. This is predominantly due to the high class imbalance present in the data. In addition to this, the amount of data used in train is very less and might not have captured enough signal to be able to identify class 1 from class 0. Even if the model recommends majority of observations as predicted purchase, the model hits decent accuracy but low values of precision and recall suggest model hasn't learnt segregation of categories well enough.

Despite this, a random guess would be correct 348/(348+5474) = 5.9% times whereas using boosting it's 22%. Hence it's 4x improvement over a random classifier.

```{r warning=FALSE}
#Benchmarking with logistic regression
logitModelCarv = glm(Purchase~., data = carv.train, family = binomial)
logitProbs = predict(logitModelCarv, carv.test, type="response")

# Confusion matrix
confusionMatrix(reference = as.factor(carv.test$Purchase), as.factor(as.numeric(logitProbs >= 0.2)))


```

We see logistic model offers a better Recall of 0.20 which is a lift from 0.11 of the boosting model. However, the precision further drops since we're recommending more number of observations herein and making a lot more of Flase positives or type I errors. Precision drops from 22% to 14%, meaning still a 2.2x increase in predictability than a random choice.


### Problem #1 : Beauty Pays!

#### Part (a)
```{r}
#Read in the data
beauty <- data.table::fread("BeautyData.csv")

#Quick summary stats
DescTools::Desc(beauty)

```
```{r}
#Modelling beauty variable into course ratings
lmModel_beauty <- lm(data = beauty , formula = CourseEvals ~ BeautyScore)
summary(lmModel_beauty)
```
Univariate linear regression points towards very strong positive association between beauty metric and the course evaluations. However beauty alone does not explain much of the variance of CourseEvals since we see a poor R-sq of 0.1657. This means we need to have a multivariate analysis of other determinant variables that might also be predictive of CourseEvals and gauge the coefficient of BeautyScore in such a setting, keeping all other determinants constant.

```{r}
#Modelling all variables into course ratings
lmModel_beautyall <- lm(data = beauty , formula = CourseEvals ~ .)
summary(lmModel_beautyall)


```

With the multivariate analysis we see all variables have significant effect on CourseEvals with beauty being one of them as well. The directionality of beauty variable co-efficient does not change sign means given other determinants such as gender, non-english, lower and tenuretrack kept constant, beauty score still has significant association with course evaluations.

#### Part (b)

While even the multivariate analysis gives out significant betas for Beauty Score, since we can't attribute association to causation we can really not say anything about whether higher beauty score is causing higher course evaluations leading to discrimination. It may very well happen that some confounding variable unused in the analysis is the reason for the strong direct relationship with BeautyScore. 

Case in point, it could be that the calculation of beauty score leverages how consistent or structured people are in their lifestyle and maintaining their appearance is only a part of their behaviour in general. Regression model using such a metric while shows direct relationship with beauty score, in reality could mean direct relationship with having structured / disciplined lifestyle.

This is where Dr. Hamermesh is suggesting that disentangling this relationship as being driven by positive confounding factors such as productivity or taking variable at face value and assuming it to be driven by discrimination based on beauty perception is probably impossible! 


### Problem #2 : Housing Price Structure

```{r}
housing <- read.csv("MidCity.csv", header = T)

#Quick data summary
summary(housing)

```

#### Part (1)

```{r}
#Multivariate analysis to model price with respect to other variables

#remove identifier column
housing$Home <- NULL

#Brick = 1 indicates Brick houses
housing$Brick <- as.factor(housing$Brick)

#Convert Neighborhood into factor
housing$Nbhd <- as.factor(housing$Nbhd)

linearModel_housing <- lm(Price ~ . , data = housing)
summary(linearModel_housing)

```

We see keeping all other co-variates constant, we see significant direct relationship between price of a house and it being a brick-house, owing to p-value way below alpha (0.05)


#### Part (2)

Since Neighbourhood goes into the model matrix as a factor with all it's levels one hot encoded, we see the model does not revert out a beta corresponding to the last level of the factor. This means, given all things constant, say if the average price of an apartment in neighborhood 3 is $100k, we can use the betas of other neighbourhoods to compare prices with neighborhood 3. Since beta for neighborhood 1 and 2 are both ~ -\$20 to -\$22k, we can say the average prices in those neighborhoods, given other covariates kept constant would be approximately \$80k and \$78k respectively.

Thus there is a premium associated with houses in neighborhood 3.

To summarize brick houses are approximately \$17k more expensive whereas neighbourhood 3 houses are roughly \$20k to\$22k more expensive.

Please note - we're assuming linear relationship between price and number of rooms and bathrooms based on intuitive sense check despite the number of unique values in these variables is not too high.

#### Part (3)

```{r}
#To check for premium for both bricked and neighborhood 3 house, we explore the interaction effects of the two variables

linearModel_housing <- lm(Price ~ Offers + SqFt + Bedrooms + Bathrooms + Brick:Nbhd , data = housing)
summary(linearModel_housing)


```

From the above summary table, we see co-efficients for all combinations of the neighborhood and whether it's a brick house are negative and significant at significance level alpha = 0.05. The level of interest is bricked house in neighborhood 3, since it's the last category in our model matrix, it gets dropped off and the predicted price for this would be equal to the intercept value given all other covariates are zero.

Since all other combinations are having negative significant betas, we can access that the there is a premium associated with Neighborhood 3 brick houses. 

On avaerage all other houses are priced approximately -\$33k less than brick houses in neighborhood 3. Note this is not equal to the sum of differences we availed from LR without interactions (~\$37k to \$39k) and this is because the effect size of one variable say neighborhood depends upon the value of other variable - house being brick house.

Hence there's a premium on neighborhood 3 brick house, but there's no extra premium as in it's not greater than the summation of premiums from linear model without interactions.

#### Part (4)

By definition in given problems, neighborhoods 1 and 2 are both traditional house settings. Seeing coefficients of these neighborhoods in previous regression result, we see betas for neighborhood 1 and 2 are pretty much similar (even though each category has significant p-value)
So, while there is definite mathematical difference between betas for the two neighborhoods, visual inspection suggests the extent of this difference is not too different compared to other parameters such as house being brick-house or not.

Hence we experiment with a model with neighborhoods 1 and 2 combined and accept it if it reduces the adjusted R-sq. This will help us avail a simpler model more robust to correctly predict the unseen test data.

```{r}

housing$NewNbhd <- ifelse(housing$Nbhd == 3,1,0)

linearModel_housing_new <- lm(Price ~  Offers + SqFt + Bedrooms + Bathrooms + Brick:NewNbhd , data = housing)
summary(linearModel_housing_new)
```

Given we do not see any improvement in adjusted R sq. and coefficients of neighborhood 1 and 2 were significant, it does not make sense to combine them into one category!

### Problem #3 : What causes what??

#### Part (1)

Different cities will have different co-efficients for their relationship of crime with respect to number of police and could potentially lead to averaging out the individual effects in case multiple cities are taken at once. For example, consider city 1 where regression line slope suggests that crime increases by certain rate with respect number of police. On the contrary city2 could have a regression line slope suggesting crime decreasing by the same rate with respect to the number of police. In case we plot these two cities data together, it results in cancellation of individual effects and the overall regression line for the two cities might result out with slop zero suggesting no relationship, which is definitely fallacious given our premise.


#### Part (2)

Given the issue is quantifying crime with respect to police is that wherever there is crime, only those places would be followed up by higher police personnel. It wasn't feasible to avail a comparison set of measuring crime rate with different number of police personnel while keeping all other variables constant since you wouldn't expect a large police personnel to be in an area without reasons other than crime.

Researchers at UPenn first isolated a controlled demographic (geography of Washington DC) first where such a case would be possible. Given DC is considered high risk area to terror attacks, they have excess police personnel whenever there is an orange alert in the city. This means researchers are able to compare crime rates on days of normal vs increased police personnel in the city while keeping any possible confounding variables constant. This data would then be helpful in isolating the effect of police personnel on crime, given all other covariates such as geography, people on the street were checked to be almost constant.

In Table 2, we quantify this when we see daily crime in DC decreased by roughly 7.3 in when modeled while not controlling for metro activity. Whereas it decreased by 6 when controlled for Metro ridership as well. The second model provides more information for predicting crime rate and increases R-square (goodness of fit) from 0.14 to 0.17. The low values of R-sq also suggest that while excess police due to high alert and absence of people from metro ridership both have a significant impact in reducing the crime rate, these do not fully explain the crime rate owing to such low amounts of variance being explained by these variables.

#### Part (3)

The hypothesis behind metro ridership was that a higher amount of police personnel would discourage public outings, which in turn would discourage any criminals to target public in their criminal activities. Public targets being available might become the cause of difference in crime on normal vs excess police days. Owing to this it becomes essential to control for the people on the streets or 'access to criminal activity'

The researchers took the metro ridership as a proxy for people travelling/in public and being prone to criminal activity and validated that this activity was the same for normal vs excess police days. This approach ensures, if all other possible reasons that could have caused a change in crime rate are kept the same, whatever increase or decrease we see in crime rate on excess police days can be directly attributed to excess police presence!

In the table, the variable undergoes Log transformation to curtail the amount of deviations.


#### Part (4)

The table 4 gives out coefficients with High alert (Excess police personnel) interacting with the concerned demographic. We see two interaction terms in the coefficient table that say while effect of High alert is significant and is reducing crime rate in District 1, we can't say anything concrete about the High alert impact on crime rate in Other districts, given the associated p-val is not sufficiently low enough. 

Adding one more covariate despite not giving out significant p-value reduces the absolute value of betas for the variables returning significant coefficients. The directionality of the coefficients in maintained as in the previous table.


### Problem #4 : Neural networks

```{r}
set.seed(100)

data <- Boston

library(neuralnet)
f <- as.formula(paste("crim ~", paste(names(train_scale)[!names(train_scale) %in% "crim"], collapse = " + ")))

set.seed(100)
cv.error <- NULL
k <- 10

library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
  
    maxs <- apply(data, 2, max) 
    mins <- apply(data, 2, min)
    
    scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))

    indexTrain <- sample(1:nrow(data),round(0.75*nrow(data)))
    train.cv <- scaled[indexTrain,]
    test.cv <- scaled[-indexTrain,]
    
    nn <- neuralnet(f,data=train.cv,hidden=c(6),linear.output=T)
    plot(nn)
    pr.nn <- compute(nn,test.cv[!names(test.cv) %in% crim])
    print("dd")
    pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
    
    test.cv.r <- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
    
    cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)
    
    pbar$step()
}

mean(cv.error)

boxplot(cv.error,xlab='MSE CV',col='cyan',
        border='blue',names='CV error (MSE)',
        main='CV error (MSE) for NN',horizontal=TRUE)

```


```{r}



```

```{r}



```